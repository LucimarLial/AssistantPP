---Table tb_log_operation to record the provenance of the execution of preprocessing operators on the columns of the dataset

-- Table: public.tb_log_operation

-- DROP TABLE public.tb_log_operation;

CREATE TABLE public.tb_log_operation
(
    id serial primary key,
    number_workflow integer,
    name_dataset character varying(500) COLLATE pg_catalog."default",
    name_column character varying(500) COLLATE pg_catalog."default",
    function_operator character varying(500) COLLATE pg_catalog."default",
    name_operator character varying(500) COLLATE pg_catalog."default",
    type_operator character varying(500) COLLATE pg_catalog."default",
    "timestamp" timestamp without time zone NOT NULL DEFAULT CURRENT_TIMESTAMP
    
)
WITH (
    OIDS = FALSE
)
TABLESPACE pg_default;

ALTER TABLE public.tb_log_operation
    OWNER to postgres;


---Necessary to ensure consistency of data, because the problem of duplicate records was detected for each reflesh of the Assistant-PP, Framework Streamlit bug.

CREATE OR REPLACE FUNCTION drop_duplicates() RETURNS trigger AS $$
BEGIN
	DELETE FROM tb_log_operation
	WHERE id IN (SELECT id
				  FROM (SELECT id,
								 ROW_NUMBER() OVER (partition BY name_dataset, name_column, function_operator, name_operator, type_operator ORDER BY id) AS rnum
						 FROM tb_log_operation) t
				  WHERE t.rnum > 1);
	RETURN NULL;
END;
$$ LANGUAGE plpgsql;


DROP TRIGGER IF EXISTS drop_duplicates
  ON tb_log_operation;
CREATE TRIGGER trig_drop_duplicates 
AFTER INSERT
ON tb_log_operation
    FOR EACH ROW EXECUTE PROCEDURE drop_duplicates();
